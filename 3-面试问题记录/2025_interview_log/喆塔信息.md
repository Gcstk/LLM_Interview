20250513面试记录

1. 你做过rag或者微调吗? 什么时候用微调,什么时候用rag,你是怎么去构想的
    ```
    RAG：适用于需要动态引入外部知识的场景（如问答系统、知识库查询）。通过检索器从知识库提取相关内容，生成器结合上下文生成答案。优点是灵活、可扩展，适合知识更新频繁或领域广泛的任务。

    微调：适用于需要模型深度适配特定任务或数据集的场景（如文本分类、特定风格生成）。通过在目标数据集上调整模型参数，提升任务性能。适合数据量足够、任务明确的情况。


    动态引入外部知识的场景
    数据量:检查是否有足够标注数据支持微调，或是否有结构化/非结构化知识库支持RAG
    速度+效果:
    特定风格生成: 这样子场景: MLCC 出现焊接不良现象 或者说 MLCC产线哪儿有问题,是怎么一步步排查的
    ```

2. lora微调的时候r=8,或者16有什么区别?你是怎么确定用哪个的?
    ```
    现在反思一下其实不是一个参数解决的,还有一个lora_alpha(一般是2*r,4*r),毕竟是 lora_alpha/r来计算的,r当然还是单独也重要,毕竟矩阵分解的那个小边,变大其实会使得你的lora模型变大,效果好一点,但是本身就是lora了,你要是很大那我不如全量微调了

    低秩分解矩阵（ΔW = A·B）进行微调，r 是低秩矩阵的秩，表示适配能力。

    简单任务（如单领域分类）用较低 r（如 8）；复杂任务（如多任务生成）用较高 r（如 16 或 32）
    ```
1. gradient_accumulation_steps梯度累计步数解释一下
   ```
   通过多次小批量（mini-batch）前向/反向传播累积梯度，模拟大批量（large batch）训练的效果
   ```
2. rag里面的知识怎么结构化的?
   ```
    读取+Chunking+过滤+tag+存储,检索,重排,生成
   ```
3. pdf,word,ppt这样的文件处理方式
4. r1这样的模型强化学习方面知识你了解多少,解释一下,有哪些指标之类的?
5. 分布式训练里面deepspeed使用介绍一下,zero1,zero2区别是什么?
    ```
    核心功能：ZeRO（Zero Redundancy Optimizer）、模型并行、管道并行、混合精度训练、梯度
    ZeRO-1：
    仅分割优化器状态（如 Adam 的动量和方差）。
    每个 GPU 仍持有完整模型参数和梯度。
    显存节省较少，适合中小模型或显存充足场景。
    ZeRO-2：
    分割优化器状态和梯度，模型参数仍完整。
    进一步减少显存占用（梯度按需通信），适合更大模型。
    通信开销略增，但显存效率更高。
    ```
6. agent智能体介绍一下你的做法,工作流介绍一下你用过的经验

本质就是工作流加function calling
小模型分类+校验,大模型处理

直接代码展示,不赘述

```python
import autogen
from langchain.document_loaders import TextLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.embeddings import HuggingFaceEmbeddings
from langchain.vectorstores import FAISS
import requests

# Step 1: Set up RAG (Knowledge Base)
def setup_knowledge_base(faq_file_path):
    # Load FAQ document
    loader = TextLoader(faq_file_path)
    documents = loader.load()
    
    # Split documents into chunks
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)
    chunks = text_splitter.split_documents(documents)
    
    # Create embeddings and store in FAISS
    embeddings = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")
    vector_store = FAISS.from_documents(chunks, embeddings)
    return vector_store

# Step 2: Mock CRM API
def get_customer_data(customer_id):
    # Mock CRM API call
    mock_data = {
        "123": {"name": "John Doe", "email": "john@example.com", "orders": ["order_001"]},
        "456": {"name": "Jane Smith", "email": "jane@example.com", "orders": ["order_002"]}
    }
    return mock_data.get(customer_id, {"error": "Customer not found"})

def update_customer_data(customer_id, data):
    # Mock CRM update
    return {"status": "success", "customer_id": customer_id, "updated_data": data}

# Step 3: Define Agent Functions
def retrieve_knowledge(query, vector_store):
    docs = vector_store.similarity_search(query, k=3)
    return "\n".join([doc.page_content for doc in docs])

# Step 4: Configure AutoGen Agents
llm_config = {
    "config_list": [
        {
            "model": "gpt-4",  # Replace with your LLM (e.g., Grok, LLaMA)
            "api_key": "your-api-key",  # Set your API key
        }
    ]
}

# Customer Service Agent
customer_service_agent = autogen.AssistantAgent(
    name="CustomerServiceAgent",
    llm_config=llm_config,
    system_message="You are a helpful customer service agent. Use the knowledge base and CRM data to assist users."
)

# Knowledge Retrieval Agent
knowledge_agent = autogen.AssistantAgent(
    name="KnowledgeAgent",
    llm_config=llm_config,
    system_message="You retrieve relevant information from the FAQ knowledge base."
)

# CRM Agent
crm_agent = autogen.AssistantAgent(
    name="CRMAgent",
    llm_config=llm_config,
    system_message="You handle CRM operations like fetching or updating customer data."
)

# Supervisor Agent
supervisor_agent = autogen.AssistantAgent(
    name="SupervisorAgent",
    llm_config=llm_config,
    system_message="You coordinate tasks among agents and ensure coherent responses."
)

# Step 5: Define Workflow
def customer_service_workflow(user_query, customer_id, vector_store):
    # Initialize group chat
    group_chat = autogen.GroupChat(
        agents=[customer_service_agent, knowledge_agent, crm_agent, supervisor_agent],
        messages=[],
        max_round=10
    )
    manager = autogen.GroupChatManager(groupchat=group_chat, llm_config=llm_config)
    
    # Start conversation
    initial_message = f"""
    User query: {user_query}
    Customer ID: {customer_id}
    Tasks:
    1. Retrieve relevant FAQ information.
    2. Fetch customer data from CRM.
    3. Provide a comprehensive response.
    """
    
    # Supervisor delegates tasks
    manager.initiate_chat(
        supervisor_agent,
        message=initial_message
    )
    
    # Mock implementation of agent interactions
    knowledge_result = retrieve_knowledge(user_query, vector_store)
    crm_data = get_customer_data(customer_id)
    
    # Combine results
    response = f"""
    Based on your query: {user_query}
    FAQ Information: {knowledge_result}
    Customer Data: {crm_data}
    How can I assist you further?
    """
    return response

# Step 6: Run the Agent
if __name__ == "__main__":
    # Setup knowledge base
    vector_store = setup_knowledge_base("faq.txt")  # Replace with your FAQ file
    
    # Example user query
    user_query = "How do I return a product?"
    customer_id = "123"
    
    # Execute workflow
    response = customer_service_workflow(user_query, customer_id, vector_store)
    print(response)
```
7. 还给了一个算法题
```
珂珂喜欢吃香蕉。这里有 n 堆香蕉，第 i 堆中有 piles[i] 根香蕉。警卫已经离开了，将在 h 小时后回来。
珂珂可以决定她吃香蕉的速度 k （单位：根/小时）。每个小时，她将会选择一堆香蕉，从中吃掉 k 根。
如果这堆香蕉少于 k 根，她将吃掉这堆的所有香蕉，然后这一小时内不会再吃更多的香蕉。
珂珂喜欢慢慢吃，但仍然想在警卫回来前吃掉所有的香蕉。
返回她可以在 h 小时内吃掉所有香蕉的最小速度 k（k 为整数）。

示例 1：
输入：piles = [3,6,7,11], h = 8
输出：4

示例 2：
输入：piles = [30,11,23,4,20], h = 5
输出：30

示例 3：
输入：piles = [30,11,23,4,20], h = 6
输出：23

提示：
1 <= piles.length <= 104
piles.length <= h <= 109
1 <= piles[i] <= 109
```
垃圾题目,不做解答,只记录
