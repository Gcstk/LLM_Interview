20250529面试记录

一共3个面试官,摄像头都不开,妈的,死刑

面试形式：小鱼易连视频面试
为保障您的面试体验，请提前安装小鱼易连..

他妈的弱智软件,安装了没法使用,导致只能小程序入会,而且这个垃圾玩意儿,还只能手机小程序,电脑的不行,,死刑,妈的,反复执行

导致我没录音,下面题目可能不全和带主观


1. 讲一下模型架构
2. moe框架细节
3. 怎么加速推理,从模型架构和部署之后分别讲一下
4. 通过什么框架推理的?怎么优化
5. kvcache
6. 说一下微调的具体场景以及细节
7. agent怎么做?
8. 文档怎么处理的,rag的每一个细节怎么做的
9. 推理加速,推理加速,怎么极致使用显卡
10. 怎么上百台机器分布式部署模型

岗位太垃圾了

---

| 组件 | 功能说明 |
|------|----------|
| **Router（路由器）** | 输入一个 token 或其表示，输出对各个 expert 的权重（概率），从中选择 top-k 个。 |
| **Experts（专家）** | 多个独立的神经网络模块（通常是前馈层 FFN）。每个专家可以有不同的容量或结构。 |
| **Top-K 选择** | 每次只激活 k 个专家（k << 总数），节省计算资源。 |
| **门控机制（Gating）** | 决定哪些专家被激活，以及它们的权重如何分配。 |


- **软路由（Soft Routing）**：使用 softmax 对所有专家打分，然后进行加权求和。
- **硬路由（Hard Routing）**：只选 top-k 个专家，其余不参与计算（更高效）。

> 通常使用 **top-2 gating + load balancing loss** 来平衡训练中的负载分布，避免某些专家被过度使用。
> 
```
        +-----------------------------+
        |         输入 token          |
        +------------+----------------+
                     |
                     v
        +----------------------------+
        |     Embedding 层 (词向量)  |
        +------------+---------------+
                     |
                     v
        +----------------------------+
        |       MoE Router 路由器    |
        |  计算每个专家对该 token 的权重 | ← Gate 在这里
        +------------+---------------+
                     |
             +-------+--------+------+-----+ ...
             |                |            |
             v                v            v
   +----------------+  +----------------+  +----------------+
   | Expert 1 (FFN) |  | Expert 2 (FFN) |  | Expert N (FFN) |
   | 专家模块 #1      |  | 专家模块 #2      |  | 专家模块 #N      |
   +----------------+  +----------------+  +----------------+

             |                |            |
             +-------+--------+------+-----+
                     |                |
                     v                |
        +----------------------------+ 
        |      权重加权融合层           |
        | 根据 router 输出的 top-k 权重 |
        | 加权组合选中的专家输出结果     |
        +------------+----------------+
                     |
                     v
        +----------------------------+
        |       输出 token 表示       |
        +-----------------------------+
```

```python
class Expert(nn.Module):
    def __init__(self, d_model):
        super().__init__()
        self.net = nn.Sequential(
            nn.Linear(d_model, d_model * 4),
            nn.ReLU(),
            nn.Linear(d_model * 4, d_model)
        )

    def forward(self, x):
        return self.net(x)

class MoELayer(nn.Module):
    def __init__(self, num_experts, d_model, k=2):
        super().__init__()
        self.experts = nn.ModuleList([Expert(d_model) for _ in range(num_experts)])
        self.router = nn.Linear(d_model, num_experts)
        self.k = k  # 激活 top-k 个专家

    def forward(self, x):
        B, T, D = x.shape
        x_flat = x.view(-1, D)  # (B*T, D)

        # Step 1: 路由器生成 logits
        logits = self.router(x_flat)  # (B*T, E)

        # Step 2: 选择 top-k 专家
        scores, indices = torch.topk(logits, self.k, dim=-1)  # (B*T, K)
        scores = F.softmax(scores, dim=-1).unsqueeze(-1)  # (B*T, K, 1)

        # Step 3: 初始化输出
        out = torch.zeros_like(x_flat)  # (B*T, D)

        # Step 4: 遍历每个专家，仅对其被选中的 token 进行计算
        for i, expert in enumerate(self.experts):
            # 找出当前专家被选中的位置
            mask = (indices == i)  # (B*T, K)
            if mask.any():
                input_masked = x_flat[mask.any(dim=-1)]  # 获取对应的 token
                output_masked = expert(input_masked)     # 专家处理
                weight_masked = scores[mask].view(-1, 1) # 权重
                out[mask.any(dim=-1)] += (output_masked * weight_masked).sum(dim=0)

        return out.view(B, T, D)
```

---

推理加速方法 vs 效果

| 方法 | 是否影响效果 | 加速比 | 显存优化 | 实现难度 | 所属阶段 | 说明 |
|------|---------------|--------|-----------|------------|----------|------|
| MoE 架构 | 否（合理设计） | 高 | 高 | 中 | 架构设计 | 激活部分专家，节省计算 |
| GQA/MQA | 否 | 中 | 高 | 中 | 架构设计 | Key/Value头共享，优化Attention |
| KV Cache 优化 | 否 | 高 | 高 | 低 | 架构设计 | 避免重复计算Key/Value |
| 动态批处理 | 否 | 高 | 中 | 中 | 部署运行 | 合并多个请求，提升利用率 |
| 量化（INT8/GGUF） | 轻微下降 | 高 | 高 | 低 | 架构设计 | 降低权重精度，节省内存 |
| 分布式张量并行 | 否 | 高 | 高 | 高 | 架构设计 | 多卡切分模型 |
| Triton/FlashAttention | 否 | 高 | 中 | 高 | 架构设计 | 利用GPU特性优化Attention |
| 流水线并行 | 否 | 中 | 中 | 高 | 部署运行 | 层级拆分模型，多设备并行 |
| 多线程/多进程并发推理 | 否 | 中 | 中 | 低 | 部署运行 | CPU调度多个推理任务 |
| Prompt Caching | 否 | 中 | 高 | 高 | 部署运行 | 缓存共享prompt内容 |
| TensorRT / ONNX Runtime | 否 | 高 | 高 | 高 | 部署运行 | 利用编译器自动优化计算图，融合算子，加速推理 |
| 异步解码 + 束搜索优化 | 否 | 中 | 中 | 高 | 部署运行 | 生成任务提速 |
| LoRA 微调（推理） | 否 | 中 | 高 | 中 | 架构设计 | 轻量加载，节省资源 |

### 1. **组合使用效果最佳**
- 比如：
  - KV Cache + GQA + FlashAttention → 显著提升Attention速度
  - 动态批处理 + 分布式张量并行 → 提升多卡吞吐
  - 量化 + TensorRT → 推理速度快且省显存

### 2. **硬件适配也很重要**
- 不同显卡（如A100 vs RTX 3090）对FP16/INT8的支持不同，选择合适精度策略
- Triton、FlashAttention 等需要特定CUDA版本和架构支持

---

### ✅ 异步解码（Asynchronous Decoding）

#### 📌 通俗解释：
正常情况下，生成文本是**一步一步来的**：每生成一个字，都要等前面的完全算完。  
而“异步解码”就是让GPU在计算当前字的同时，提前准备下一个字的输入，就像“边做饭边备料”。

#### 🔍 技术说明：
- 利用CUDA流（CUDA Streams）实现多个任务并发执行。
- 将数据搬运、计算、调度等操作错开执行，避免空闲等待。

### ✅ 束搜索优化（Beam Search Optimization）

#### 📌 通俗解释：
你要写一句话，不是只选“最可能”的那个字，而是同时尝试多个可能性（比如5个），最后选出最好的整句。这就是“束搜索”。  
但这个过程很慢，所以要优化它，比如：
- 减少重复计算
- 提前剪枝（Pruning）
- 并行化多个候选路径

```
┌────────────┐   ┌────────────┐   ┌────────────┐
│ 第1个token │   │ 第2个token │   │ 第3个token │
└──────┬─────┘   └──────┬─────┘   └──────┬─────┘
       │                │                │
       ▼                ▼                ▼
  [异步启动]        [异步启动]        [异步启动]
       │                │                │
       ▼                ▼                ▼
  [解码+评分]       [解码+评分]       [解码+评分]
       │                │                │
       ▼                ▼                ▼
[更新候选序列]   [更新候选序列]   [更新候选序列]

          \              |             /
           \             |            /
            -----> [束搜索剪枝] <-----

每个token的生成和评分都是异步进行的，最终通过“束搜索剪枝”保留最优路径。
```
