20250528面试记录

今天面了好几家,每一个都是90分钟..日,真的累,身体都受不了了,幸好老子女朋友对我好



一共四个人面试,晚上7:00-8:30左右,日

那个懂技术的一直怼老子,你来面试,他妈的,老子随便问你几个你都回答不上来

其他人还可以,都比较温柔

1. 你用过哪些成熟的推理框架?
2. 大模型推理指标tmtp..(弱智,妈的)这个面试的
3. 怎么评价推理的框架的好坏?有什么指标
4. 全参数微调,lora微调有什么区别吗,具体冻结了哪些层的参数
    
    | 对比维度 | 全参数微调（Full Fine-tuning） | LoRA 微调 |
    |----------|-----------------------------|-----------|
    | **是否更新所有参数？** | ✅ 是，更新整个模型的所有参数 | ❌ 否，只更新新增的小型低秩矩阵 |
    | **训练参数量** | 非常大（例如 LLaMA-7B：70 亿参数） | 极小（通常仅几万到几十万参数） |
    | **训练速度 & 显存消耗** | 慢，显存占用高 | 快，显存占用低 |
    | **模型泛化能力** | 更好（理论上），但容易过拟合 | 表现接近，但更稳定、轻便 |
    | **部署与保存多个版本模型** | 大文件，难管理 | 小文件，便于多任务切换 |

    Lora: QKV 投影层（Attention 中的 q_proj, k_proj, v_proj）最常见且有效

1. 微调框架怎么选的,有什么指标依据吗?
2. deepspeed的不同阶段
    ```
    ZeRO（Zero Redundancy Optimizer）：
    这是 DeepSpeed 的核心创新，分为几个阶段（ZeRO-1、ZeRO-2、ZeRO-3）：
    ZeRO-1：只分割优化器状态（如动量和方差），降低显存需求。
    ZeRO-2：进一步分割梯度，减少通信量。
    ZeRO-3：分割模型参数、梯度和优化器状态，动态调度显存，极大降低单卡显存需求，同时支持超大规模模型训练。
    ```
3. 解释一下量化的awq,gguf等他们的差别
4. python元类解释一下
5. python代码怎么做高并发
6. 文生图sd,midjourney,flux他们prompt有什么特别的区别吗?
7. 在选择工作的时候你会更考虑哪些因素呢?
8. 3词评价自己
9. 微调的具体场景
10. 智能体这个你是怎么做的?
