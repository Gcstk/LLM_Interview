### What it is
Transformer 是一种神经网络结构，它利用了自注意力（self-attention）机制和多层编码器（encoder）与解码器（decoder）层
从而有效地处理长距离依赖关系和捕获不同层次的文本信息。

步骤
```text
Input Embeddings
Positional Encodings
Layer Normalization
Feed Forward
Multi-Head Attention
Residual Connection
Encoder
Decoder
Linear Layer
Transformer
Task overview
Tokenizer
Dataset
Training loop
Validation loop
Attention visualization
```




### Reference(参考文档)
[论文:Attention Is All You Need](..%2Fusing_files%2Fpaper%2Fpytorch2transformer.pdf)
* [论文](https://arxiv.org/abs/1706.03762)
* [PyTorch搭建Transformer视频](https://www.youtube.com/watch?v=ISNdQcPhsts&ab_channel=UmarJamil)
* [上述视频的文档github地址](https://github.com/aceliuchanghong/pytorch-transformer)
