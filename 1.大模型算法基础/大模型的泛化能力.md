1. 大模型常用的激活函数有哪些？

```text
ReLU（Rectified Linear Unit）：一种简单的激活函数，可以解决梯度消失问题，加快训练速度。
GeLU（Gaussian Error Linear Unit）：一种改进的ReLU函数，可以提供更好的性能和泛化能力
Swish：一种自门控激活函数，可以提供非线性变换
```

2. LLMs 复读机问题

```text
1.大型语言模型在生成文本时会重复之前已经生成的内容
2.LLMs 复读机问题可能由多种因素引起，包括模型训练数据中的重复模式、
模型在处理长序列时的注意力机制失效、或者模型在生成文本时对过去信息的过度依赖等
```

3. 如何让大模型处理更长的文本？

```text
使用内存机制，如外部记忆或缓存，来存储和检索长文本中的信息。
使用分块方法，将长文本分割成更小的部分，然后分别处理这些部分。
```

4. 目前主流的开源模型体系有哪些？

```text
GPT（Generative Pre-trained Transformer）系列
BERT（Bidirectional Encoder Representations from Transformers）
T5（Text-to-Text Transfer Transformer）
```

5. prefix LM 和 causal LM 区别是什么？

```text
Prefix LM（前缀语言模型）和Causal LM（因果语言模型）是两种不同类型的语言模型
它们的区别在于生成文本的方式和训练目标。
Prefix LM：
前缀语言模型是一种生成模型，它在生成每个词时都可以考虑之前的上下文信息。
在生成时，前缀语言模型会根据给定的前缀（即部分文本序列）预测下一个可能的词。
这种模型可以用于文本生成、机器翻译等任务。

Causal LM：
因果语言模型是一种自回归模型，它只能根据之前的文本生成后续的文本，而不能根据后续的文本生成之前的文本。
在训练时，因果语言模型的目标是预测下一个词的概率，给定之前的所有词作为上下文。
这种模型可以用于文本生成、语言建模等任务。

总结来说，前缀语言模型可以根据给定的前缀生成后续的文本，而因果语言模型只能根据之前的文本生成后续的文本。
GPT系列就是Causal LM
```

6. LLMs Tokenizer 介绍

```text
大模型的Tokenizer通常使用字节对编码（Byte-Pair Encoding，BPE）算法
是一种常用的无监督分词方法，用于将文本分解为子词或字符级别的单位。BPE的词典构建过程如下：

初始化词典：
{'h', 'e', 'l', 'o', ' ', 'w', 'r', 'd'}
统计频率：
统计语料库中每个字符或子词出现的频率。对于"hello world"这个示例文本，统计频率如下：
h: 1, e: 1, l: 3, o: 2, ' ': 1, w: 1, r: 1, d: 1

合并频率最高的相邻字符：
通过合并频率最高的相邻字符，可以得到新的子词"el"，更新词典如下：
{'h', 'e', 'l', 'o', ' ', 'w', 'r', 'd', 'el'}

更新文本：
替换后的文本为："h el lo world"

重复步骤2至4：
重复执行步骤2至4，直到达到预先设定的合并次数或者满足其他停止条件。

一种可能的合并序列是：
合并频率最高的相邻字符 "el"，得到新的子词 "el"。
合并频率最高的相邻字符 "el" 和 "lo"，得到新的子词 "elo"。
更新后的词典：
{'h', 'e', 'l', 'o', ' ', 'w', 'r', 'd', 'el', 'elo'}

构建最终词典：
经过预设的合并次数或者其他停止条件后，得到的词典即为最终的词典。
{'h', 'e', 'l', 'o', ' ', 'w', 'r', 'd', 'el', 'elo'}

最终得到的词典即为BPE的词典。通过BPE算法，可以将文本分解为多个子词，其中一些子词可能是常见的词汇，而其他子词则是根据输入文本的特点生成的。
这种方式可以更好地处理未登录词和稀有词，并提高模型对复杂词汇和短语的处理能力。
```

7. RLHF 在实践过程中存在哪些不足？

```text
RLHF（Reinforcement Learning from Human Feedback）
是一种通过人类反馈进行增强学习的方法，在实践过程中仍然存在以下几个不足之处：
1.人类反馈的代价高昂 2.错误反馈的影响
RLHF 的基本思想是，代理系统与人类交互，并根据人类的反馈来调整其行为。
这种反馈可以是明确的，比如标签化的反馈（如“好”、“坏”）
```

8. Self-attention的公式及参数量？为什么要除以根号d？

```text
将得分除以根号D（得分归一化）可以防止内积过大导致softmax函数梯度变得非常小，
这有助于数值稳定性，使得学习过程更加稳定。
此外，它还可以看作是一种缩放因子，帮助模型在不同维度上保持一致的性能。
```

![img.png](..%2Fusing_files%2Fimg%2Fllms%2Fimg.png)

### Reference(参考文档)

* [大模型通用问题](https://zhuanlan.zhihu.com/p/683078370)
* [github问题仓库](https://github.com/aceliuchanghong/others_interview_notes)
