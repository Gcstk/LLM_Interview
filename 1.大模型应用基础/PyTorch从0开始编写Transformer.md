### What it is
Transformer 是一种神经网络结构，它利用了自注意力（self-attention）机制和多层编码器（encoder）与解码器（decoder）层
从而有效地处理长距离依赖关系和捕获不同层次的文本信息。

步骤
```text
Input Embeddings
Positional Encodings
Layer Normalization
Feed Forward
Multi-Head Attention
Residual Connection
Encoder
Decoder
Linear Layer
Transformer
Task overview
Tokenizer
Dataset
Training loop
Validation loop
Attention visualization
```

待写(我还没学完,之后写):


### Reference(参考文档)
[论文:Attention Is All You Need](..%2Fusing_files%2Fpaper%2Fpytorch2transformer.pdf)
* [论文](https://arxiv.org/abs/1706.03762)
* [PyTorch搭建Transformer视频](https://www.youtube.com/watch?v=ISNdQcPhsts&ab_channel=UmarJamil)
* [上述视频的文档github地址](https://github.com/aceliuchanghong/pytorch-transformer)
* [Pytorch面试题整理](https://blog.csdn.net/qq_43687860/article/details/132795944)
* [视觉算法工程师面经](https://blog.csdn.net/ly59782/article/details/120671350?utm_medium=distribute.pc_relevant.none-task-blog-2~default~baidujs_baidulandingword~default-9-120671350-blog-119749474.235^v43^control&spm=1001.2101.3001.4242.6&utm_relevant_index=12)
* [算法工程师面经](https://blog.csdn.net/julyedu_7/article/details/122473408?spm=1001.2101.3001.6650.1&utm_medium=distribute.pc_relevant.none-task-blog-2%7Edefault%7ECTRLIST%7ERate-1-122473408-blog-119749474.235%5Ev43%5Econtrol&depth_1-utm_source=distribute.pc_relevant.none-task-blog-2%7Edefault%7ECTRLIST%7ERate-1-122473408-blog-119749474.235%5Ev43%5Econtrol&utm_relevant_index=2)
